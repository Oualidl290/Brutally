#!/usr/bin/env python3
"""
src/api/main.py
Main FastAPI application entry point - Following Enterprise Architecture
"""

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from prometheus_fastapi_instrumentator import Instrumentator

from .routes import jobs, processing, downloads, health, metrics
from .middleware import auth, rate_limit, logging as log_middleware
from .websockets import progress, notifications
from ..utils.constants import API_VERSION, APP_NAME
from ..database.connection import init_database
from ..services.cache_service import CacheService


def create_app() -> FastAPI:
    """Application factory"""
    
    app = FastAPI(
        title=APP_NAME,
        version=API_VERSION,
        docs_url="/api/docs",
        redoc_url="/api/redoc",
        openapi_url="/api/openapi.json"
    )
    
    # Initialize database
    init_database()
    
    # Initialize cache
    CacheService.initialize()
    
    # Add middleware
    setup_middleware(app)
    
    # Add routes
    setup_routes(app)
    
    # Add WebSocket endpoints
    setup_websockets(app)
    
    # Setup metrics
    setup_metrics(app)
    
    # Mount static files
    app.mount("/static", StaticFiles(directory="web/dist"), name="static")
    
    return app


def setup_middleware(app: FastAPI):
    """Configure middleware"""
    
    # CORS
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],  # Configure appropriately for production
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    
    # Custom middleware
    app.add_middleware(log_middleware.LoggingMiddleware)
    app.add_middleware(rate_limit.RateLimitMiddleware)
    app.add_middleware(auth.AuthMiddleware)


def setup_routes(app: FastAPI):
    """Configure API routes"""
    
    # Health checks
    app.include_router(
        health.router,
        prefix="/api/health",
        tags=["health"]
    )
    
    # Job management
    app.include_router(
        jobs.router,
        prefix="/api/v1/jobs",
        tags=["jobs"]
    )
    
    # Processing endpoints
    app.include_router(
        processing.router,
        prefix="/api/v1/process",
        tags=["processing"]
    )
    
    # Download endpoints
    app.include_router(
        downloads.router,
        prefix="/api/v1/downloads",
        tags=["downloads"]
    )
    
    # Metrics endpoint
    app.include_router(
        metrics.router,
        prefix="/api/metrics",
        tags=["metrics"]
    )


def setup_websockets(app: FastAPI):
    """Configure WebSocket endpoints"""
    
    app.add_api_websocket_route(
        "/ws/progress/{job_id}",
        progress.websocket_endpoint
    )
    
    app.add_api_websocket_route(
        "/ws/notifications",
        notifications.websocket_endpoint
    )


def setup_metrics(app: FastAPI):
    """Setup Prometheus metrics"""
    
    Instrumentator().instrument(app).expose(app)


# Create app instance
app = create_app()


# -----------------------------------------------------------
# src/api/routes/processing.py
# -----------------------------------------------------------

from typing import List
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from uuid import uuid4

from ..models.requests import ProcessingRequest, VideoURL
from ..models.responses import JobResponse, ProcessingStatus
from ..middleware.auth import get_current_user
from ...services.processing_service import ProcessingService
from ...workers.tasks.processing_tasks import process_videos_task
from ...database.repositories.job_repo import JobRepository
from ...utils.validators import validate_urls, validate_quality

router = APIRouter()


@router.post("/", response_model=JobResponse)
async def create_processing_job(
    request: ProcessingRequest,
    background_tasks: BackgroundTasks,
    current_user=Depends(get_current_user),
    processing_service: ProcessingService = Depends(),
    job_repo: JobRepository = Depends()
):
    """Create a new video processing job"""
    
    # Validate request
    if not validate_urls(request.urls):
        raise HTTPException(status_code=400, detail="Invalid URLs provided")
    
    if not validate_quality(request.quality):
        raise HTTPException(status_code=400, detail="Invalid quality setting")
    
    # Create job
    job_id = str(uuid4())
    job = await job_repo.create(
        job_id=job_id,
        user_id=current_user.id,
        request_data=request.dict()
    )
    
    # Queue processing task
    task = process_videos_task.apply_async(
        args=[job_id, request.dict()],
        priority=request.priority
    )
    
    # Update job with task ID
    await job_repo.update(job_id, {"task_id": task.id})
    
    return JobResponse(
        job_id=job_id,
        status=ProcessingStatus.PENDING,
        message="Job queued successfully",
        created_at=job.created_at,
        eta=processing_service.estimate_time(request)
    )


@router.get("/{job_id}/status", response_model=ProcessingStatus)
async def get_processing_status(
    job_id: str,
    current_user=Depends(get_current_user),
    processing_service: ProcessingService = Depends()
):
    """Get processing job status"""
    
    status = await processing_service.get_status(job_id)
    if not status:
        raise HTTPException(status_code=404, detail="Job not found")
    
    # Check ownership
    if status.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Access denied")
    
    return status


@router.post("/{job_id}/cancel")
async def cancel_processing_job(
    job_id: str,
    current_user=Depends(get_current_user),
    processing_service: ProcessingService = Depends()
):
    """Cancel a processing job"""
    
    result = await processing_service.cancel_job(job_id, current_user.id)
    if not result:
        raise HTTPException(status_code=404, detail="Job not found or cannot be cancelled")
    
    return {"message": "Job cancelled successfully"}


# -----------------------------------------------------------
# src/api/models/requests.py
# -----------------------------------------------------------

from pydantic import BaseModel, Field, HttpUrl, validator
from typing import List, Optional
from enum import Enum


class VideoQuality(str, Enum):
    P480 = "480p"
    P720 = "720p"
    P1080 = "1080p"
    P2160 = "2160p"


class CompressionPreset(str, Enum):
    ULTRAFAST = "ultrafast"
    FAST = "fast"
    MEDIUM = "medium"
    SLOW = "slow"
    VERYSLOW = "veryslow"


class VideoURL(BaseModel):
    """Video URL model"""
    url: HttpUrl
    episode_number: int = Field(..., ge=1, le=999)
    title: Optional[str] = None
    
    @validator('url')
    def validate_url(cls, v):
        # Add custom URL validation if needed
        return v


class ProcessingRequest(BaseModel):
    """Video processing request model"""
    urls: List[VideoURL] = Field(..., min_items=1, max_items=100)
    season_name: str = Field(..., min_length=1, max_length=100)
    quality: VideoQuality = VideoQuality.P1080
    compression_preset: CompressionPreset = CompressionPreset.MEDIUM
    compression_level: int = Field(default=23, ge=18, le=28)
    use_gpu: bool = True
    use_hardware_accel: bool = True
    priority: int = Field(default=5, ge=1, le=10)
    notification_webhook: Optional[HttpUrl] = None
    tags: Optional[List[str]] = []
    
    class Config:
        schema_extra = {
            "example": {
                "urls": [
                    {"url": "https://example.com/video1.mp4", "episode_number": 1},
                    {"url": "https://example.com/video2.mp4", "episode_number": 2}
                ],
                "season_name": "Season1",
                "quality": "1080p",
                "compression_preset": "medium",
                "use_gpu": True,
                "priority": 5
            }
        }


# -----------------------------------------------------------
# src/services/processing_service.py
# -----------------------------------------------------------

from typing import List, Optional
from pathlib import Path
import asyncio

from ..core.downloader import DownloadManager, DownloadConfig
from ..core.processor import VideoProcessor, ProcessingConfig
from ..core.compressor import IntelligentCompressor
from ..core.merger import VideoMerger
from ..database.repositories.job_repo import JobRepository
from ..services.storage_service import StorageService
from ..services.cache_service import CacheService
from ..services.notification_service import NotificationService
from ..hardware.gpu_detector import GPUDetector
from ..utils.exceptions import ProcessingError


class ProcessingService:
    """Main video processing orchestration service"""
    
    def __init__(
        self,
        download_manager: DownloadManager,
        video_processor: VideoProcessor,
        compressor: IntelligentCompressor,
        merger: VideoMerger,
        job_repo: JobRepository,
        storage_service: StorageService,
        cache_service: CacheService,
        notification_service: NotificationService
    ):
        self.download_manager = download_manager
        self.video_processor = video_processor
        self.compressor = compressor
        self.merger = merger
        self.job_repo = job_repo
        self.storage_service = storage_service
        self.cache_service = cache_service
        self.notification_service = notification_service
        self.gpu_detector = GPUDetector()
    
    async def process_season(
        self,
        job_id: str,
        urls: List[str],
        season_name: str,
        config: dict
    ) -> Path:
        """Process entire season of videos"""
        
        try:
            # Update job status
            await self.job_repo.update_status(job_id, 'initializing')
            
            # Setup directories
            work_dir = await self.storage_service.create_work_directory(job_id)
            download_dir = work_dir / "downloads"
            processed_dir = work_dir / "processed"
            
            # Phase 1: Download
            await self.job_repo.update_status(job_id, 'downloading')
            download_config = DownloadConfig(
                use_aria2c=True,
                resume_enabled=True,
                max_connections=16
            )
            
            download_results = await self.download_manager.download_batch(
                urls=urls,
                output_dir=download_dir,
                config=download_config,
                job_id=job_id
            )
            
            # Phase 2: Process
            await self.job_repo.update_status(job_id, 'processing')
            
            # Detect available hardware
            gpu_info = await self.gpu_detector.detect()
            processing_config = ProcessingConfig(
                use_gpu=config.get('use_gpu', True) and gpu_info['available'],
                gpu_type=gpu_info.get('type'),
                threads=config.get('threads', 8),
                preset=config.get('compression_preset', 'medium')
            )
            
            processed_files = await self.video_processor.process_batch(
                input_files=[r.local_path for r in download_results],
                output_dir=processed_dir,
                config=processing_config,
                job_id=job_id
            )
            
            # Phase 3: Merge and Compress
            await self.job_repo.update_status(job_id, 'merging')
            
            output_file = await self.merger.merge_videos(
                video_files=processed_files,
                output_path=work_dir / f"{season_name}.mp4",
                compress=True,
                compression_level=config.get('compression_level', 23)
            )
            
            # Phase 4: Upload to final storage
            await self.job_repo.update_status(job_id, 'uploading')
            
            final_path = await self.storage_service.move_to_final_storage(
                output_file,
                f"completed/{season_name}.mp4"
            )
            
            # Update job completion
            await self.job_repo.update_status(job_id, 'completed')
            await self.job_repo.update(job_id, {
                'output_file': str(final_path),
                'completed_at': datetime.utcnow()
            })
            
            # Send notification
            if config.get('notification_webhook'):
                await self.notification_service.send_completion_notification(
                    job_id,
                    config['notification_webhook']
                )
            
            # Cleanup temporary files
            await self.storage_service.cleanup_work_directory(work_dir)
            
            return final_path
            
        except Exception as e:
            await self.job_repo.update_status(job_id, 'failed')
            await self.job_repo.add_error(job_id, str(e))
            raise ProcessingError(f"Processing failed: {e}")
    
    async def get_status(self, job_id: str) -> Optional[dict]:
        """Get job status"""
        job = await self.job_repo.get(job_id)
        if not job:
            return None
        
        # Get progress from cache
        progress = await self.cache_service.get(f"progress:{job_id}")
        
        return {
            'job_id': job_id,
            'status': job.status,
            'progress': progress or 0,
            'created_at': job.created_at,
            'updated_at': job.updated_at,
            'errors': job.errors,
            'output_file': job.output_file
        }
    
    async def cancel_job(self, job_id: str, user_id: str) -> bool:
        """Cancel a processing job"""
        job = await self.job_repo.get(job_id)
        
        if not job or job.user_id != user_id:
            return False
        
        if job.status in ['completed', 'failed']:
            return False
        
        # Revoke Celery task
        from celery import current_app
        if job.task_id:
            current_app.control.revoke(job.task_id, terminate=True)
        
        # Update status
        await self.job_repo.update_status(job_id, 'cancelled')
        
        # Cleanup
        work_dir = Path(f"/tmp/video_processor/{job_id}")
        if work_dir.exists():
            await self.storage_service.cleanup_work_directory(work_dir)
        
        return True
    
    def estimate_time(self, request: dict) -> int:
        """Estimate processing time in seconds"""
        # Simple estimation based on number of videos and quality
        base_time = 60  # Base time per video
        num_videos = len(request.get('urls', []))
        
        quality_multiplier = {
            '480p': 0.5,
            '720p': 0.8,
            '1080p': 1.0,
            '2160p': 2.0
        }.get(request.get('quality', '1080p'), 1.0)
        
        gpu_multiplier = 0.3 if request.get('use_gpu') else 1.0
        
        estimated = int(base_time * num_videos * quality_multiplier * gpu_multiplier)
        return estimated